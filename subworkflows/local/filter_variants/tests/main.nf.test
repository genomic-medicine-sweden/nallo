nextflow_workflow {

    name "Test Workflow FILTER_VARIANTS"
    script "../main.nf"
    config "./nextflow.config"
    workflow "FILTER_VARIANTS"

    setup {
        run("GUNZIP") {
            script "../../../../modules/nf-core/gunzip/main.nf"
            process {
                """
                input[0] = [
                    [ id:'hg38' ],
                    file(params.pipelines_testdata_base_path + 'reference/hg38.test.fa.gz', checkIfExists: true)
                ]
                """
            }
        }
        run("SAMTOOLS_FAIDX") {
            script "../../../../modules/nf-core/samtools/faidx/main.nf"
            process {
                """
                input[0] = GUNZIP.out.gunzip
                input[1] = [[],[]]
                input[2] = false
                """
            }
        }

        run("UNTAR") {
            script "../../../../modules/nf-core/untar/main.nf"
            process {
                """
                input[0] = [
                    [ id: 'vep_cache' ],
                    file(params.pipelines_testdata_base_path + 'reference/vep_cache_test_data.tar.gz', checkIfExists:true)
                ]
                """
            }
        }
        run("ANNOTATE_SNVS") {
            script "../../annotate_snvs/main.nf"
            process {
                """
                input[0] = Channel.of([
                    [ id: 'test' ],
                    file(params.pipelines_testdata_base_path + 'testdata/NIST_deepvariant_norm.vcf.gz', checkIfExists: true)
                ])
                input[1] = [
                    file(params.pipelines_testdata_base_path + 'reference/cadd.v1.6.hg38.test_data.zip', checkIfExists: true)
                ]
                input[2] = GUNZIP.out.gunzip
                input[3] = SAMTOOLS_FAIDX.out.fai
                input[4] = UNTAR.out.untar.map { meta, cache -> cache}
                input[5] = Channel.value('110')
                input[6] = Channel.of([
                    file(params.pipelines_testdata_base_path + 'reference/vep_plugin_files.csv', checkIfExists: true)
                ]).splitCsv(header:true).map { row -> row.vep_files }.collect()
                input[7] = false
                input[8] = true
                input[9] = Channel.value([])
                input[10] = null
                input[11] = null
                input[12] = false
            """
            }
        }
    }

    test("vcf, [[],[]], false") {

        when {
            workflow {
                """
                input[0] = ANNOTATE_SNVS.out.vcf
                input[1] = Channel.of([[],[]])
                input[2] = false
                """
            }
        }

        then {
            assertAll(
                { assert workflow.success },
                { assert snapshot(
                    workflow.out.versions,
                    file(workflow.out.tbi.get(0).get(1)).name,
                    path(workflow.out.vcf.get(0).get(1)).vcf.summary,
                ).match() }
            )
        }
    }

    test("vcf, hgnc_ids, true") {
        tag "hgnc"
        when {
            workflow {
                """
                input[0] = ANNOTATE_SNVS.out.vcf
                input[1] = Channel.of('HGNC:4826')
                    .collectFile(name: 'hgnc_ids.txt')
                    .map { file -> [ [ id: 'hgnc_ids' ], file ] }
                input[2] = true
                """
            }
        }

        then {
            assertAll(
                { assert workflow.success },
                { assert snapshot(
                    workflow.out.versions,
                    file(workflow.out.tbi.get(0).get(1)).name,
                    path(workflow.out.vcf.get(0).get(1)).vcf.summary,
                ).match() }
            )
        }
    }
}
